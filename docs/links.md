# ğŸ”— Links and References

Links to research papers and resources corresponding to implemented features in this repository. Please feel free to fill in any missing references!

---

## ğŸ“Œ Table of Contents
- [Implemented Methods](#implemented-methods)
- [Benchmarks](#benchmarks)
- [Evaluation Metrics](#evaluation-metrics)
- [Useful Links](#useful-links)
  - [Survey Papers](#survey-papers)
  - [Other GitHub Repositories](#other-github-repositories)

---

## ğŸ“— Implemented Methods

| Method          | Resource |
|-----------------|----------|
| GradAscent, GradDiff | Naive baselines found in many papers including MUSE, TOFU etc. |
| NPO             | Paper [ğŸ“„](https://arxiv.org/abs/2404.05868), Code [ğŸ™](https://github.com/licong-lin/negative-preference-optimization) |
| SimNPO             |  Paper [ğŸ“„](https://arxiv.org/abs/2410.07163), Code [ğŸ™](https://github.com/OPTML-Group/Unlearn-Simple) |
| IdkDPO             | TOFU ([ğŸ“„](https://arxiv.org/abs/2401.06121)) |
| RMU             | WMDP paper ([ğŸ™](https://github.com/centerforaisafety/wmdp/tree/main/rmu), [ğŸŒ](https://www.wmdp.ai/)), later used in G-effect ([ğŸ™](https://github.com/tmlr-group/G-effect/blob/main/dataloader.py)) |

---

## ğŸ“˜ Benchmarks

| Benchmark | Resource |
|-----------|----------|
| TOFU      | Paper [ğŸ“„](https://arxiv.org/abs/2401.06121) |
| MUSE      | Paper [ğŸ“„](https://arxiv.org/abs/2407.06460) |

---

## ğŸ“™ Evaluation Metrics

| Metric | Resource |
|--------|----------|
| Verbatim Probability / ROUGE, simple QA-ROUGE | Naive metrics found in many papers including MUSE, TOFU etc. |
| Membership Inference Attacks (LOSS, ZLib, Reference, GradNorm, MinK, MinK++) | MIMIR ([ğŸ™](https://github.com/iamgroot42/mimir)), MUSE ([ğŸ“„](https://arxiv.org/abs/2407.06460)) |
| PrivLeak | MUSE ([ğŸ“„](https://arxiv.org/abs/2407.06460)) |
| Forget Quality, Truth Ratio, Model Utility | TOFU ([ğŸ“„](https://arxiv.org/abs/2401.06121)) |
| Extraction Strength (ES) |  Carlini et al., 2021 ([ğŸ“„](https://www.usenix.org/conference/usenixsecurity21/presentation/carlini-extracting)), used for unlearning in Wang et al., 2025 ([ğŸ“„](https://openreview.net/pdf?id=wUtCieKuQU)) |
| Exact Memorization (EM) |  Tirumala et al., 2022 ([ğŸ“„](https://proceedings.neurips.cc/paper_files/paper/2022/hash/fa0509f4dab6807e2cb465715bf2d249-Abstract-Conference.html)), used for unlearning in Wang et al., 2025 ([ğŸ“„](https://openreview.net/pdf?id=wUtCieKuQU)) |

---

## ğŸŒ Useful Links

### ğŸ“š Surveys
- [Machine Unlearning in 2024](https://ai.stanford.edu/~kzliu/blog/unlearning)
- [Rethinking Machine Unlearning for Large Language Models](https://arxiv.org/abs/2402.08787)

### ğŸ™ Other GitHub Repositories
- [TOFU Benchmark (original)](https://github.com/locuslab/tofu)
- [MUSE Benchmark (original)](https://github.com/swj0419/muse_bench)
- [Awesome LLM Unlearning](https://github.com/chrisliu298/awesome-llm-unlearning)
- [Awesome Machine Unlearning](https://github.com/tamlhp/awesome-machine-unlearning)
- [Awesome GenAI Unlearning](https://github.com/franciscoliu/Awesome-GenAI-Unlearning)